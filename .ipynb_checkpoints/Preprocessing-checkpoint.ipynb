{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQzFd_RxzX0B"
   },
   "source": [
    "# Preprocessing\n",
    "The aim of this code is to generate a preprocessed VQA V2 dataset in binary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlMhcZGszTbB"
   },
   "outputs": [],
   "source": [
    "#VQA V2 images directory\n",
    "train_images_root = ''\n",
    "val_images_root = ''\n",
    "#Directories where to put the result after train and validation preprocessing.\n",
    "train_feats_root = ''\n",
    "val_feats_root = ''\n",
    "#VQA V2 train questions file\n",
    "train_questions_file = ''\n",
    "val_questions_file\n",
    "#VQA V2 val questions file\n",
    "train_answers_file = ''\n",
    "val_answers_file = ''\n",
    "#VQA complementary pair files : https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Complementary_Pairs_Train_mscoco.zip\n",
    "pairs_file = ''\n",
    "drive_root = ''\n",
    "#number of rows in each serialized file\n",
    "FILE_LENGTH = 100 * 40\n",
    "#Batch length for generating preprocessed data\n",
    "RESNET_BATCH_SIZE = 40\n",
    "#minumum occurrence of words in questions\n",
    "NUM_OCCURENCE = 5\n",
    "#initalised in answerPreprocessing\n",
    "ANSWER_DIM = 0\n",
    "\n",
    "import os\n",
    "import importlib.util\n",
    "from google.colab import drive\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import time\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "\n",
    "tf.enable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzpYhFJMHaWf"
   },
   "source": [
    "# Mount drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uEwnOwc217eJ"
   },
   "outputs": [],
   "source": [
    "# Run this cell to mount your Google Drive.\n",
    "from google.colab import drive\n",
    "drive.mount(drive_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oGAmJNdKHjz4"
   },
   "source": [
    "# Question Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMAZnq8C_2aV"
   },
   "outputs": [],
   "source": [
    "SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    sentence = sentence.strip().lower()\n",
    "    sentence = (\n",
    "        sentence.replace(',', '').replace('?', '').replace('\\'s', ' \\'s'))\n",
    "    tokens = SENTENCE_SPLIT_REGEX.split(sentence)\n",
    "    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aweZi-lv9D2X"
   },
   "outputs": [],
   "source": [
    "class QuestionsPreprocessing : \n",
    "  \n",
    "  def __init__(self, glove_file, questions_files, num_occurence) :\n",
    "    self.gloveFile  = glove_file\n",
    "    self.questionsFiles = questions_files\n",
    "    self.word2Glove = self.readGloveFile()\n",
    "    questions = self.readQuestionsFiles()\n",
    "    self.questions_files = questions_files\n",
    "    #filter words that have occurence less than occurence\n",
    "    words = self.filter_words(questions,num_occurence)\n",
    "    #partition words from weather they are in glove or not\n",
    "    self.word2Index, self.index2Word = self.matchWordIndex(words)\n",
    "    #consider the 0 padding\n",
    "    self.vocab_length = len(self.word2Index) + 1 \n",
    "     \n",
    "    \n",
    "  def readGloveFile(self):\n",
    "      with open(self.gloveFile, 'r') as f:\n",
    "          wordToGlove = {}  # map from a token (word) to a Glove embedding vector\n",
    "          #wordToIndex = {}  # map from a token to an index\n",
    "          #indexToWord = {}  # map from an index to a token \n",
    "\n",
    "          for line in f:\n",
    "              record = line.strip().split()\n",
    "              token = record[0] # take the token (word) from the text line\n",
    "              wordToGlove[token] = np.array(record[1:], dtype=np.float64) # associate the Glove embedding vector to a that token (word)\n",
    "\n",
    "      return wordToGlove\n",
    "    \n",
    "  #return all words in questions\n",
    "  #WARNING IMPORTANT : THIS FUNCTION IS ONLY FOR RETRIEVING QUESTIONS FROM VQA DATASET\n",
    "  def readQuestionsFiles(self) : \n",
    "    questions = []\n",
    "    for file in self.questionsFiles :\n",
    "      with open(file, 'r') as f : \n",
    "        data = json.load(f)\n",
    "        for x in data['questions']:\n",
    "          questions.append(x['question'])\n",
    "    return questions    \n",
    "        \n",
    "    \n",
    "  def filter_words(self,questions, num_occurence):\n",
    "    words = {}\n",
    "    for question in questions : \n",
    "      for word in tokenize(question):\n",
    "        if(word in words):\n",
    "          words[word] +=1\n",
    "        else :\n",
    "          words[word] = 1\n",
    "    return [k for k,v in words.items() if v > num_occurence]\n",
    "   \n",
    "    \n",
    "  def matchWordIndex(self, words) : \n",
    "    word2Index = {}\n",
    "    index2Word = {}\n",
    "    for i, word in enumerate(words):\n",
    "      word2Index[word] = i+1\n",
    "      index2Word[i+1] =word\n",
    "    return word2Index, index2Word\n",
    "    \n",
    " \n",
    "  # create embedding matrix\n",
    "  def createPretrainedEmbeddingLayer(self):\n",
    "      wordToIndex = self.word2Index\n",
    "      wordToGlove = self.word2Glove\n",
    "      vocabLen = len(wordToIndex) + 1  # adding 1 to account for masking\n",
    "      glove_words = wordToGlove.keys()\n",
    "      embDim = next(iter(wordToGlove.values())).shape[0] \n",
    "      embeddingMatrix = np.zeros((vocabLen, embDim), 'float64')  # initialize with zeros\n",
    "      for word, index in wordToIndex.items():\n",
    "        if word in glove_words:\n",
    "          embeddingMatrix[index, :] = wordToGlove[word] # create embedding: word index to Glove word embedding\n",
    "        else : \n",
    "          embeddingMatrix[index, :] = np.random.rand(1, embDim)\n",
    "      return embeddingMatrix\n",
    "\n",
    "  \n",
    "  def get_embedding_matrix(self) : \n",
    "    \n",
    "    embedding_matrix = self.createPretrainedEmbeddingLayer()\n",
    "    return embedding_matrix\n",
    "  \n",
    "# TODO : check if there is another method like removing most common words  \n",
    "  def preprocessBatch(self,questions, max_len):\n",
    "    batch = []\n",
    "    for question in questions : \n",
    "      words =self.preprocessElem(question)\n",
    "      batch.append(num_words)\n",
    "    return self.postTruncate(batch, max_len)\n",
    "\n",
    "  def preprocessElem(self,question):\n",
    "    words = self.preTruncate(question)\n",
    "    words = tokenize(words)\n",
    "    num_words = []\n",
    "    for word in words :\n",
    "      w = self.word2Index.get(word, 'not_found_word')\n",
    "      if not w is 'not_found_word' :\n",
    "        num_words.append(w)\n",
    "    return num_words\n",
    "  #private\n",
    "  #TODO : implement\n",
    "  def preTruncate(self, words): \n",
    "    return words\n",
    "  \n",
    "  def postTruncate(self,batch, max_len) : \n",
    "    return  keras.preprocessing.sequence.pad_sequences(batch, max_len,padding ='post', truncating = 'post')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1sdt5rBHd2U"
   },
   "source": [
    "# Answer preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SzJ6vxi_8iJR"
   },
   "outputs": [],
   "source": [
    "class AnswerPreprocessing : \n",
    "  def __init__(self, questions_files, answers_files):\n",
    "    global ANSWER_DIM\n",
    "    assert len(questions_files) == len(answers_files), 'not the same length'\n",
    "    self.answers, _ = self.get_answers(questions_files[0], answers_files[0])\n",
    "    #for i, f in enumerate(questions_files):\n",
    "     # s,m = self.get_answers(questions_files[i], answers_files[i])\n",
    "     # self.answers = self.answers.union(s)\n",
    "    self.word2Index = {}\n",
    "    self.index2Word = {}\n",
    "    self.word2Index, self.index2Word = self.matchWords2Indexes()\n",
    "    ANSWER_DIM = len(self.word2Index)\n",
    "    self.num_words = len(self.word2Index)\n",
    "  \n",
    "  def get_dim():\n",
    "    return len(self.word2Index)\n",
    "  #private\n",
    "  def matchWords2Indexes(self): \n",
    "    wordToIndex = {}\n",
    "    indexToWord = {}\n",
    "    words = self.answers\n",
    "    words = self.filterWords(words)\n",
    "    wordToIndex['no idea'] = 0\n",
    "    indexToWord[0] = 'no idea'\n",
    "    i = 1\n",
    "    for w in words:\n",
    "      if not w == 'no idea' :\n",
    "        wordToIndex[w] = i\n",
    "        indexToWord[i] = w\n",
    "        i += 1\n",
    "    return (wordToIndex, indexToWord)\n",
    "  #private\n",
    "  def readWords(self) :\n",
    "    words = []\n",
    "    for file in self.answer_files : \n",
    "      with open(file, 'r') as f : \n",
    "        data = json.load(f)\n",
    "        for annotation in data['annotations'] : \n",
    "          for answer in annotation['answers']:\n",
    "            words.append(answer['answer'])\n",
    "    return words\n",
    "  \n",
    "  #private\n",
    "  def filterWords (self, words) : \n",
    "    return words\n",
    "  \n",
    "  def preprocessBatch(self, answers) : \n",
    "    batch  = []\n",
    "    ans = np.array(answers)\n",
    "    if ans.ndim == 2 : \n",
    "      for answers2 in answers : \n",
    "         batch.append([self.word2Index[x] for x in answers2])\n",
    "    else : \n",
    "      for answer in answers: \n",
    "        batch.append(self.word2Index[answer])\n",
    "    return batch\n",
    " \n",
    "  \n",
    "  def _preprocessElem(self,ans):\n",
    "    # if the dataset contains multiple answers : Exemple VQA dataset\n",
    "    answer = np.array([x for x in ans if not x == FILL_TOKEN])\n",
    "    if answer.ndim == 1 :\n",
    "      arr = np.zeros((ANSWER_DIM,))\n",
    "      if len(answer) == 0 :\n",
    "        arr[0] = 1.0\n",
    "        return arr\n",
    "      else :\n",
    "      #arr = np.zeros(self.num_words,dtype=int)\n",
    "        \n",
    "        value = 1/len(answer)\n",
    "        found = False\n",
    "        for i, a in enumerate(answer):\n",
    "          if a in self.word2Index:\n",
    "            found = True\n",
    "            arr[self.word2Index[a]] += value\n",
    "        if not found :\n",
    "          arr[0] = 1.0\n",
    "        return arr\n",
    "    #if the answer is unique in the dataset\n",
    "    else :\n",
    "      return self.word2Index[answer]\n",
    "    \n",
    "  def preprocessElem(self,ans):\n",
    "    answer = np.array([x for x in ans if not x == FILL_TOKEN], 'str')\n",
    "    arr = np.zeros((ANSWER_DIM,))\n",
    "    if len(answer) == 0 :\n",
    "      arr[0] =1\n",
    "    else :\n",
    "      c = Counter(answer)\n",
    "      t = [x for (x,y) in c.items() if x in self.word2Index]\n",
    "      if t == [] :\n",
    "        arr[0] = 1\n",
    "      else :\n",
    "        for elem, occur in c.items() :\n",
    "          if elem in self.word2Index :\n",
    "            index = self.word2Index[elem]\n",
    "            score = 1 if occur >=3 else 1/3* occur\n",
    "            arr[index] = score\n",
    "    return arr \n",
    "  \n",
    "  def get_ques2(self,questions_file):\n",
    "      with open(questions_file,'r') as f : \n",
    "        data = json.load(f)\n",
    "      questions = data['questions']\n",
    "      qsid_iq = { x['question_id']:  x['question'] for x in questions }\n",
    "      return qsid_iq\n",
    "\n",
    "    \n",
    "  def get_answers( self, questions_file,answers_file) :\n",
    "    global ANSWER_DIM\n",
    "    numbers  = [\"zero\",\"one\",\"two\",\"three\",\"four\",\n",
    "          \"five\",\"six\",\"seven\",\"eight\",\"nine\",\"ten\",\n",
    "          \"eleven\",\"twelve\",\"thirteen\",\"fourteen\",\"fifteen\",\n",
    "          \"sixteen\",\"seventeen\",\"eighteen\",\"nineteen\"];\n",
    "    tens = [\"Twenty\",\"Thirty\",\"Forty\",\"Fifty\",\n",
    "          \"Sixty\",\"Seventy\",\"Eighty\",\"Ninety\"]\n",
    "    id_qs = self.get_ques2(questions_file)\n",
    "\n",
    "    with open(answers_file,'r') as f : \n",
    "      data = json.load(f)\n",
    "    resps = data['annotations']\n",
    "    res = [] \n",
    "    ids = []\n",
    "    conf =[]\n",
    "    for x in resps:\n",
    "      question = id_qs[x['question_id']].strip().lower()\n",
    "      questionID = x['question_id']\n",
    "      ans = [y['answer'].replace(',', ' ').replace('?', '').replace('\\'s', ' \\'s').strip().lower() for y in x['answers'] ]\n",
    "      res1 =[]\n",
    "      for word in ans :\n",
    "          if word == 'no 1' or 'no one' in word :\n",
    "            res1.append('no one')\n",
    "          elif word in ['no clue', \"i dont know\", \"i don't know\", \"cannot know\", \"can't know\", \"can't tell\", \"not sure\", \"don't know\", \"cannot tell\", \"unknown\"]:\n",
    "            res1.append('no idea')\n",
    "          elif word == 'my best guess is no' or \"it isn't\" in word or  'it is not' in word:\n",
    "            res1.append('no')\n",
    "          elif 'many' in word or 'several' in word or 'lot' in word or 'numerous' in word:\n",
    "            res1.append('many')\n",
    "          elif word in numbers :\n",
    "            res1.append(str(numbers.index(word)))\n",
    "          elif word in tens:\n",
    "            res1.append(str((ten.index(word) + 2) * 10))\n",
    "          else :\n",
    "            res1.append(word)\n",
    "\n",
    "\n",
    "\n",
    "      if question.startswith('how many') or question.startswith('what is the number'):\n",
    "        for word in res1 :\n",
    "          if re.search('(\\s|^)no ', word) or re.search(' no(\\s|$)',word):\n",
    "            if word == 'no idea':\n",
    "              res.append('no idea')        \n",
    "            else :\n",
    "              res.append('0')\n",
    "          elif word == 'o' :\n",
    "            res.append(0)\n",
    "          elif not len(re.findall('\\d+', word)) == 0:\n",
    "              res.append(re.findall('\\d+', word)[0])         \n",
    "          elif word == 'no' :\n",
    "              res.append('0')\n",
    "          elif word =='yes' :\n",
    "              res.append('1')\n",
    "          else :\n",
    "              res.append(word)\n",
    "\n",
    "      elif question.startswith('is') or question.startswith('are'):\n",
    "\n",
    "        for word in res1 :\n",
    "          if re.search('(\\s|^)no ', word) or re.search(' no(\\s|$)',word):\n",
    "            res.append('no')\n",
    "          elif word == 'it is' or 'yes' in word:\n",
    "            res.append('yes')\n",
    "          elif 'it is' in word :\n",
    "            s = word.replace('it is', '').strip()\n",
    "            res.append(s)\n",
    "            continue\n",
    "          else :\n",
    "            res.append(word) \n",
    "\n",
    "      else :\n",
    "        for word in res1 :\n",
    "          if word == 'it is' or 'yes' in word:\n",
    "              res.append('yes')\n",
    "          elif 'it is' in word :\n",
    "            s = word.replace('it is', '').strip()\n",
    "            res.append(s)\n",
    "          elif ('there is no' in word) or (\"there's no\" in word) or ('there are no' in word):\n",
    "            res.append('not found')\n",
    "          elif word.strip().startswith('no ') :\n",
    "            ans_tokens = tokenize(word[2:])\n",
    "            ques_tokens = tokenize(question)\n",
    "            boo = True\n",
    "            for t in ans_tokens:\n",
    "              if not (t in ques_tokens or t+'s' in ques_tokens):\n",
    "                boo = False\n",
    "                break\n",
    "            if boo :\n",
    "              res.append('not found')\n",
    "            else :\n",
    "              res.append(word)\n",
    "          else :\n",
    "            res.append(word)  \n",
    "\n",
    "      for s in ans:\n",
    "        ids.append(questionID)\n",
    "\n",
    "      #TODO remove this:    \n",
    "      conf1 = [y['answer_confidence'] for y in x['answers'] ]\n",
    "      conf.extend(conf1)\n",
    "\n",
    "    newres = []\n",
    "    newids = []\n",
    "    for index in range(len(res)) :\n",
    "      if conf[index] == 'yes' :\n",
    "        newres.append(res[index])\n",
    "        newids.append(ids[index])\n",
    "    c = Counter(newres)\n",
    "    resset = set([k for k,v in c.items() if v >= ANSWER_OCCURENCE_MIN])\n",
    "    m = {}\n",
    "    for index in range(len(newres)) :\n",
    "      qid = newids[index]\n",
    "      response = newres[index]\n",
    "      if  response in resset : \n",
    "        if qid in m :\n",
    "          m[qid].append(response)\n",
    "        else :\n",
    "          m[qid] = [response]\n",
    "  #  queskeys = set(ques.keys())\n",
    "\n",
    "   # m = {k : v for k,v in m.items() if k in queskeys} \n",
    "    return resset, m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IqM3-ZE4HtSh"
   },
   "source": [
    "# Dataset generation\n",
    "includes image preprocessing. Generates serialised files containing train and validation preprocessed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9T__l5zFygx1"
   },
   "outputs": [],
   "source": [
    "USED = False\n",
    "class VQA_DatasetGenerator2 : \n",
    "  def __init__(self,question_preprocessor, answer_preprocessor):\n",
    "    self.question_preprocessor = question_preprocessor\n",
    "    self.answer_preprocessor = answer_preprocessor\n",
    "    #self.answers_file = answers_file\n",
    "    #self.complementary_questions = self.__get_complementary_questions()\n",
    "\n",
    " \n",
    "  def _bytes_feature(self,value):\n",
    "\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "  def _float_feature(self,value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "  def _int64_feature(self,value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "  def _image_example(self,images_paths, questions, answers, model,file_path):\n",
    "    \n",
    "    def preprocess_image(image):\n",
    "      image = tf.image.decode_jpeg(image, channels=3)\n",
    "      image = tf.image.resize_images(image, [224, 224])\n",
    "      return image\n",
    "    def load_and_preprocess_image(path):\n",
    "      image = tf.read_file(path)\n",
    "      return preprocess_image(image)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('before dataset')\n",
    "    data = tf.data.Dataset.from_tensor_slices(images_paths).apply(tf.data.experimental.map_and_batch(\n",
    "      map_func =  load_and_preprocess_image,batch_size = RESNET_BATCH_SIZE, num_parallel_batches=5\n",
    "      )).prefetch(RESNET_BATCH_SIZE ).make_one_shot_iterator()\n",
    "\n",
    "    examples = []\n",
    "    t = time.time()\n",
    "    for ib, batch in enumerate(data) :\n",
    "        print('batch {}'.format(ib))\n",
    "        \n",
    "        imgs_feats = model(batch)\n",
    "        for ife, feat in enumerate(imgs_feats) :\n",
    "          idx = ib * RESNET_BATCH_SIZE + ife\n",
    "          answer = answers[idx]\n",
    "          question = questions[idx]\n",
    "          im = feat.numpy().flatten()\n",
    "\n",
    "          answer = answer + self.fill_list(len(answer))\n",
    "          answer = self.answer_preprocessor.preprocessElem(answer)\n",
    "          feature = {\n",
    "            'image': self._float_feature(im),\n",
    "            'question' :self._int64_feature(question),\n",
    "            'answers' :self._float_feature(answer)\n",
    "           }   \n",
    "          examples.append( tf.train.Example(features=tf.train.Features(feature=feature)) )\n",
    "\n",
    "        '''if ib % 75 == 0 : \n",
    "          print('batch {} in {}'.format(ib, time.time() - t))\n",
    "          t = time.time()\n",
    "          for example in examples:       \n",
    "            writer.write(example.SerializeToString())\n",
    "          del examples\n",
    "          examples = []'''\n",
    "    print('open file  {}'.format(file_path))  \n",
    "    with tf.python_io.TFRecordWriter(file_path) as writer:\n",
    "      for example in examples:       \n",
    "        writer.write(example.SerializeToString())\n",
    "    print('file finished')\n",
    "    del examples  \n",
    "\n",
    "     \n",
    " \n",
    "  #TODO :add reorder treatment \n",
    "  def get_data(self, image_folder, questions_file, answers_file, pairs_file,dataSubType,ext, train, reorder):\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_image_id(file_name):\n",
    "      id = file_name.split('_')[2][:-4]\n",
    "      return int(id)\n",
    "\n",
    "    def get_path(image_id, dataSubType, ext):\n",
    "      imgFilename = 'COCO_' + dataSubType + '_'+ str(image_id).zfill(12) + '.'+ext\n",
    "      return imgFilename\n",
    "\n",
    "    def get_im_ids(im_folder):\n",
    "      ims = [get_image_id(f) for f in os.listdir(im_folder)]\n",
    "      return set(ims)\n",
    "\n",
    "    def get_ques(questions_file, pairs_file, image_ids, dataSubType,ext, reorder):\n",
    "      if not USED :\n",
    "        with open(pairs_file, 'r') as f : \n",
    "          data = json.load(f)\n",
    "        pairs = [y for x in data for y in x ]\n",
    "\n",
    "        with open(questions_file) as f : \n",
    "          data = json.load(f)\n",
    "        ques = data['questions']\n",
    "\n",
    "        ques_map = {x['question_id'] : x for x in ques}\n",
    "        qids  = [x['question_id'] for x in ques]\n",
    "        diff_set = set(qids) - set(pairs)\n",
    "        for x in diff_set :\n",
    "          pairs.append(x)\n",
    "        questions = [ ques_map[x] for x in pairs]\n",
    "\n",
    "        if reorder : \n",
    "          #TODO : implement\n",
    "          print('reorder not implemented')\n",
    "          #questions = self.__order(questions, 'question_id')\n",
    "\n",
    "        '''with open('/content/gdrive/My Drive/nabih/questions.txt', 'w') as f:\n",
    "          for i, q in enumerate(pairs) :\n",
    "            if not i == 0 :\n",
    "              f.write('\\n')\n",
    "            f.write(str(q))'''\n",
    "      else :\n",
    "        with open('/content/gdrive/My Drive/nabih/questions.txt', 'r') as f:\n",
    "          pairs = [int(x) for x in f.readlines()]\n",
    "        with open(questions_file) as f : \n",
    "          data = json.load(f)\n",
    "        ques = data['questions']\n",
    "        ques_map = {x['question_id'] : x for x in ques}\n",
    "        questions = [ ques_map[x] for x in pairs]\n",
    "        \n",
    "\n",
    "      qsid_iq = { x['question_id']: [get_path(x['image_id'],dataSubType,ext), x['question']] for x in questions if x['image_id'] in image_ids }\n",
    "\n",
    "      return qsid_iq\n",
    "   \n",
    "    def gen_ans(answers_file, ques):\n",
    "      with open(answers_file) as f : \n",
    "        data = json.load(f)\n",
    "      resps = data['annotations']\n",
    "      resps = { x['question_id'] : [y['answer'] for y in x['answers']] for x in resps if int(x['question_id']) in ques}\n",
    "      return resps\n",
    "\n",
    "    #TODO : lot of word done here because j ai la flemme de tt refaire\n",
    "    \n",
    "\n",
    "    \n",
    "    ti = time.time()\n",
    "    ids = get_im_ids(image_folder)\n",
    "    print('get data : get image {}'.format(time.time() - ti))\n",
    "    ti = time.time()\n",
    "    ques = get_ques(questions_file, pairs_file, ids, dataSubType,ext, reorder)\n",
    "    print('get data : questions {} len of ques {}'.format(time.time() -  ti, len(ques)))\n",
    "    ti = time.time()\n",
    "    resset, ans = self.answer_preprocessor.get_answers(questions_file, answers_file)\n",
    "    print('get data : ans {}'.format(time.time() -  ti))\n",
    "    ti = time.time()\n",
    "       # We get the map from just the first file\n",
    "    dico = []\n",
    "    for qid , iq in ques.items():\n",
    "      if train :\n",
    "        try :\n",
    "          answers  = ans[qid]\n",
    "        except :\n",
    "          answers = []\n",
    "        t = tuple([iq[0], iq[1], answers])\n",
    "        #r a in answers :\n",
    "         #newiq =  [x for x in iq]\n",
    "         #newiq.append(a)\n",
    "         #dico.append(newiq)\n",
    "        dico.append(t)\n",
    "      else :\n",
    "        t = tuple(iq)\n",
    "        dico.append(t)\n",
    "    print('get data : dico {}'.format(time.time() -  ti))\n",
    "\n",
    "    return zip(*dico) \n",
    "  \n",
    "  def fill_list(self, leng) : \n",
    "    x = 10 - leng\n",
    "    return [FILL_TOKEN for i in range(x)]\n",
    "\n",
    "  #TODO : see if i must include the full image.:\n",
    "\n",
    "  def generate(self, image_feats_file, questions_file, answers_file ,pairs_file, dataSubType, ext, feats_root, chunk_size):\n",
    "    #im_qsids_qs = self.__get_questions(questions_file,reorder)\n",
    "    images, questions, answers = self.get_data(image_feats_file, questions_file, answers_file,pairs_file, dataSubType,ext, True,False)\n",
    "    print('get data done len of images {}'.format(len(images)))\n",
    "    images_paths =  [os.path.join(image_feats_file, d) for d in images]\n",
    "    questions = [self.question_preprocessor.preprocessElem(x) for x in questions]\n",
    "    questions = self.question_preprocessor.postTruncate(questions, MAX_LEN)\n",
    "    print('questions done')\n",
    "    model = keras.applications.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(224, 224,3), pooling=None)\n",
    "    t = time.time()\n",
    "    #for file_index, chunk_start in enumerate(range(0, len(questions), chunk_size)):\n",
    "    file_index = 65\n",
    "    start = file_index*chunk_size\n",
    "    t = time.time()\n",
    "    last = len(questions)\n",
    "    for chunk_start in range(start, last, chunk_size) :\n",
    "      print('in chunk')\n",
    "      print('total time {}'.format(time.time() - t))\n",
    "      t = time.time()\n",
    "      file_path = os.path.join(feats_root, 'TFREC_'+str(file_index)+'.tfrecord')\n",
    "      chunk_end = chunk_start + chunk_size\n",
    "      chunk_end = chunk_end if chunk_end <= last else last\n",
    "      chunk_images_paths = images_paths[chunk_start : chunk_end ]\n",
    "      chunk_questions = questions[chunk_start : chunk_end ]\n",
    "      chunk_answers = answers[chunk_start : chunk_end ]      \n",
    "      self._image_example(chunk_images_paths, chunk_questions, chunk_answers, model, file_path)\n",
    "      print('treated {}  time {}'.format(chunk_end, time.time() - t))\n",
    "      file_index +=1\n",
    "          \n",
    "        \n",
    "      \n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "        \n",
    "  #TODO\n",
    "  def __order(self, items,key): \n",
    "    return items\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJelOU0f0r88"
   },
   "outputs": [],
   "source": [
    "dataset_generator =  VQA_DatasetGenerator2(question_preprocessor, answer_preprocessor)\n",
    "answer_preprocessor = AnswerPreprocessing([train_questions_file],[train_answers_file])\n",
    "question_preprocessor = QuestionsPreprocessing(glove_file_path, [train_questions_file], NUM_OCCURENCE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hw6kFcD4IDlE"
   },
   "source": [
    "# Generate train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRQro3tB6toe"
   },
   "outputs": [],
   "source": [
    "dataset_generator.generate(train_images_root,train_questions_file, train_answers_file,pairs_file, 'train2014','jpg',train_feats_root,FILE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmkEFinHIGkd"
   },
   "source": [
    "# Generate validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOKL_BRK1m-u"
   },
   "outputs": [],
   "source": [
    "dataset_generator.generate(val_images_root,val_questions_file, val_answers_file, 'val2014','jpg',val_feats_root,FILE_LENGTH )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocessing.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
